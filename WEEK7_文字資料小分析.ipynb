{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiroqPre88+P66eRmZeIqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alayuala/114-1PL.repo/blob/main/WEEK7_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **æ–‡å­—è³‡æ–™å°åˆ†æï¼ˆå ±å‘Šè‰ç¨¿/å•å·é–‹æ”¾é¡Œï¼‰ï¼ˆä½œæ¥­å››ï¼‰**  \n",
        "* ç›®æ¨™ï¼šå¾ Sheet è®€é–‹æ”¾å¼å›ç­” â†’ åšè©æ•¸èˆ‡é—œéµå­—è¨ˆæ•¸ â†’ è¼¸å‡ºå‰ N ç†±è© â†’ å›å¯«çµ±è¨ˆè¡¨ã€‚  \n",
        "* AI é»å­ï¼šè«‹æ¨¡å‹ç”¢å‡º 5 å¥æ´å¯Ÿæ‘˜è¦ + ä¸€æ®µ 120 å­—çµè«–ã€‚  \n"
      ],
      "metadata": {
        "id": "n8tr6INakL5U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ed29ac"
      },
      "source": [
        "# å®‰è£å¿…è¦çš„ Python å¥—ä»¶\n",
        "# é€™é‚Šåˆ—å‡ºäº†æˆ‘å€‘é€™å€‹å°ˆæ¡ˆæœƒç”¨åˆ°çš„æ‰€æœ‰å¥—ä»¶\n",
        "# gspread: ç”¨ä¾†æ“ä½œ Google Sheets\n",
        "# gspread_dataframe: æ–¹ä¾¿ Pandas DataFrame èˆ‡ Google Sheets ä¹‹é–“çš„è½‰æ›\n",
        "# google-auth, google-auth-oauthlib, google-auth-httplib2: Google èº«ä»½é©—è­‰ç›¸é—œ\n",
        "# gradio: å»ºç«‹ä¸€å€‹äº’å‹•å¼çš„ç¶²é ä»‹é¢\n",
        "# pandas: å¼·å¤§çš„è³‡æ–™è™•ç†å’Œåˆ†æå·¥å…·\n",
        "# beautifulsoup4: ç”¨æ–¼è§£æ HTML ç¶²é å…§å®¹ï¼Œæ–¹ä¾¿æŠ“å–è³‡æ–™\n",
        "# google-generativeai: å‘¼å« Google çš„ç”Ÿæˆå¼ AI æ¨¡å‹ (Gemini)\n",
        "# python-dateutil: è™•ç†æ—¥æœŸå’Œæ™‚é–“ï¼Œç‰¹åˆ¥æ˜¯æ™‚å€å•é¡Œ\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb8bf7bf"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (è¨­å®šæ‚¨çš„ Google Sheet é€£çµã€å·¥ä½œè¡¨åç¨±å’Œæ™‚å€)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae4f6ef3"
      },
      "source": [
        "# è¨­å®šæ‚¨çš„ Google Sheet é€£çµã€å·¥ä½œè¡¨åç¨±å’Œæ™‚å€\n",
        "# è«‹å°‡ SHEET_URL æ›¿æ›æˆæ‚¨è‡ªå·±çš„ Google Sheet é€£çµ\n",
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/163AOmd5HnH9sxzYJo4BwZeE87UFr3k5XxuggcRNsZCs/edit?usp=sharing\" # <-- è«‹ç¢ºèªé€™æ˜¯æ‚¨çš„ Google Sheet ç¶²å€\n",
        "INPUT_WORKSHEET_NAME = \"å·¥ä½œè¡¨1\" # <-- é–‹æ”¾å¼å›ç­”è³‡æ–™æ‰€åœ¨çš„å·¥ä½œè¡¨åç¨±\n",
        "OUTPUT_WORKSHEET_NAME = \"åˆ†æçµæœ\" # <-- æ–‡æœ¬åˆ†æçµæœå°‡å¯«å…¥çš„å·¥ä½œè¡¨åç¨± (æˆ‘æœƒè‡ªå‹•å»ºç«‹)\n",
        "TIMEZONE = \"Asia/Taipei\" # è¨­å®šæ™‚å€"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3681a8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db66aca5-26bf-4513-d1a0-5c44fb4300f5"
      },
      "source": [
        "# ç¢ºä¿æŒ‡å®šçš„å·¥ä½œè¡¨ (worksheet) åœ¨è©¦ç®—è¡¨ä¸­å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºä¸€å€‹æ–°çš„\n",
        "def ensure_worksheet(sh, title, header):\n",
        "    print(f\"å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ '{title}' å­˜åœ¨...\")\n",
        "    try:\n",
        "        ws = sh.worksheet(title) # å˜—è©¦é–‹å•Ÿç¾æœ‰çš„å·¥ä½œè¡¨\n",
        "        print(f\"å·¥ä½œè¡¨ '{title}' å·²å­˜åœ¨ã€‚\")\n",
        "    except gspread.WorksheetNotFound:\n",
        "        print(f\"å·¥ä½œè¡¨ '{title}' ä¸å­˜åœ¨ï¼Œå˜—è©¦å‰µå»º...\")\n",
        "        try:\n",
        "            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå‰‡æ–°å¢ä¸€å€‹å·¥ä½œè¡¨ï¼Œä¸¦è¨­å®šåˆå§‹çš„è¡Œåˆ—æ•¸\n",
        "            ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "            print(f\"å·¥ä½œè¡¨ '{title}' å‰µå»ºæˆåŠŸã€‚\")\n",
        "            ws.update([header]) # æ–°å¢æ™‚å¯«å…¥è¡¨é ­\n",
        "            print(f\"å·²ç‚ºå·¥ä½œè¡¨ '{title}' å¯«å…¥è¡¨é ­ã€‚\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ å‰µå»ºå·¥ä½œè¡¨ '{title}' å¤±æ•—: {e}\")\n",
        "            raise # å‰µå»ºå¤±æ•—æ™‚é‡æ–°æ‹‹å‡ºç•°å¸¸\n",
        "    # å†æ¬¡æª¢æŸ¥ï¼šè‹¥æ²’æœ‰è¡¨é ­ï¼ˆå·¥ä½œè¡¨æ˜¯ç©ºçš„æˆ–ç¬¬ä¸€è¡Œä¸æ˜¯é æœŸçš„è¡¨é ­ï¼‰ï¼Œå‰‡æ¸…ç©ºä¸¦å¯«ä¸Šè¡¨é ­\n",
        "    try:\n",
        "        data = ws.get_all_values()\n",
        "        if not data or (data and data[0] != header):\n",
        "            print(f\"å·¥ä½œè¡¨ '{title}' æ²’æœ‰è¡¨é ­æˆ–è¡¨é ­ä¸ç¬¦ï¼Œæ¸…ç©ºä¸¦é‡æ–°å¯«å…¥è¡¨é ­ã€‚\")\n",
        "            ws.clear() # æ¸…ç©ºå·¥ä½œè¡¨å…§å®¹\n",
        "            ws.update([header]) # é‡æ–°å¯«å…¥è¡¨é ­\n",
        "            print(f\"å·²ç‚ºå·¥ä½œè¡¨ '{title}' é‡æ–°å¯«å…¥è¡¨é ­ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æª¢æŸ¥æˆ–æ›´æ–°å·¥ä½œè¡¨ '{title}' è¡¨é ­å¤±æ•—: {e}\")\n",
        "        # é€™è£¡é¸æ“‡ä¸ä¸­æ–·ç¨‹å¼ï¼Œä½†è¨˜éŒ„éŒ¯èª¤\n",
        "\n",
        "    print(f\"ç¢ºä¿å·¥ä½œè¡¨ '{title}' éç¨‹çµæŸã€‚\")\n",
        "    return ws\n",
        "\n",
        "# è®€å– DataFrame çš„è¼”åŠ©å‡½å¼\n",
        "def read_df(ws, header):\n",
        "    \"\"\"å¾ Google Sheet å·¥ä½œè¡¨è®€å–è³‡æ–™åˆ° DataFrameï¼Œä¸¦ç¢ºä¿æ¬„ä½å’Œå‹æ…‹æ­£ç¢º\"\"\"\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        sheet_header = ws.get_all_values('A1:1')\n",
        "        cols = sheet_header[0] if sheet_header and sheet_header[0] else header\n",
        "        return pd.DataFrame(columns=cols)\n",
        "    df = df.fillna(\"\")\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    return df\n",
        "\n",
        "# å°‡ DataFrame å¯«å…¥ Google Sheet çš„è¼”åŠ©å‡½å¼\n",
        "def write_df(ws, df, header):\n",
        "    \"\"\"å°‡ DataFrame å…§å®¹å¯«å…¥ Google Sheet å·¥ä½œè¡¨ (å…¨é‡è¦†è“‹)\"\"\"\n",
        "    if df.empty:\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "        return\n",
        "    df_out = df.copy()\n",
        "    for c in df_out.columns:\n",
        "        df_out[c] = df_out[c].astype(str)\n",
        "    ws.clear()\n",
        "    ws.update([header] + df_out[header].values.tolist())\n",
        "\n",
        "# éœ€è¦ tznow å‡½å¼ä¾†è™•ç†æ™‚é–“æˆ³è¨˜\n",
        "def tznow():\n",
        "    from datetime import datetime as dt\n",
        "    from dateutil.tz import gettz\n",
        "    # ç¢ºä¿ TIMEZONE è®Šæ•¸å·²å®šç¾©\n",
        "    try:\n",
        "        TIMEZONE\n",
        "    except NameError:\n",
        "        # å¦‚æœ TIMEZONE æœªå®šç¾©ï¼Œçµ¦å‡ºè­¦å‘Šä¸¦ä½¿ç”¨é è¨­å€¼\n",
        "        print(\"âš ï¸ æ™‚å€è®Šæ•¸ TIMEZONE æœªå®šç¾©ï¼Œè«‹åŸ·è¡Œè¨­å®šå„²å­˜æ ¼ã€‚é€™è£¡å°‡ä½¿ç”¨é è¨­å€¼ Asia/Taipeiã€‚\")\n",
        "        TIMEZONE_DEFAULT = \"Asia/Taipei\"\n",
        "        return dt.now(gettz(TIMEZONE_DEFAULT))\n",
        "    # å¦‚æœ TIMEZONE å·²å®šç¾©ï¼Œå‰‡ä½¿ç”¨å®ƒ\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "# ç¢ºä¿ PTT æ–‡ç« è³‡æ–™ã€è¼¸å…¥è³‡æ–™å’Œåˆ†æçµæœçš„å·¥ä½œè¡¨å­˜åœ¨ä¸¦æœ‰æ­£ç¢ºçš„è¡¨é ­\n",
        "# ä½¿ç”¨å‰é¢å®šç¾©çš„ ensure_worksheet å‡½å¼ä¾†è™•ç†\n",
        "print(\"ç¢ºä¿ PTT æ–‡ç« å·¥ä½œè¡¨...\")\n",
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER) # PTT æ–‡ç« å·¥ä½œè¡¨\n",
        "print(\"ç¢ºä¿ è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨...\")\n",
        "ws_input = ensure_worksheet(sh, INPUT_WORKSHEET_NAME, INPUT_HEADER) # è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨\n",
        "print(\"ç¢ºä¿ åˆ†æçµæœå·¥ä½œè¡¨...\")\n",
        "ws_analysis = ensure_worksheet(sh, OUTPUT_WORKSHEET_NAME, ANALYSIS_HEADER) # åˆ†æçµæœå·¥ä½œè¡¨\n",
        "\n",
        "print(\"æ‰€æœ‰å¿…è¦å·¥ä½œè¡¨ç¢ºä¿å®Œæˆã€‚\")"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç¢ºä¿ PTT æ–‡ç« å·¥ä½œè¡¨...\n",
            "å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ 'ptt_movie_posts' å­˜åœ¨...\n",
            "å·¥ä½œè¡¨ 'ptt_movie_posts' ä¸å­˜åœ¨ï¼Œå˜—è©¦å‰µå»º...\n",
            "å·¥ä½œè¡¨ 'ptt_movie_posts' å‰µå»ºæˆåŠŸã€‚\n",
            "å·²ç‚ºå·¥ä½œè¡¨ 'ptt_movie_posts' å¯«å…¥è¡¨é ­ã€‚\n",
            "ç¢ºä¿å·¥ä½œè¡¨ 'ptt_movie_posts' éç¨‹çµæŸã€‚\n",
            "ç¢ºä¿ è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨...\n",
            "å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' å­˜åœ¨...\n",
            "å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' å·²å­˜åœ¨ã€‚\n",
            "å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' æ²’æœ‰è¡¨é ­æˆ–è¡¨é ­ä¸ç¬¦ï¼Œæ¸…ç©ºä¸¦é‡æ–°å¯«å…¥è¡¨é ­ã€‚\n",
            "å·²ç‚ºå·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' é‡æ–°å¯«å…¥è¡¨é ­ã€‚\n",
            "ç¢ºä¿å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' éç¨‹çµæŸã€‚\n",
            "ç¢ºä¿ åˆ†æçµæœå·¥ä½œè¡¨...\n",
            "å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ 'åˆ†æçµæœ' å­˜åœ¨...\n",
            "å·¥ä½œè¡¨ 'åˆ†æçµæœ' ä¸å­˜åœ¨ï¼Œå˜—è©¦å‰µå»º...\n",
            "å·¥ä½œè¡¨ 'åˆ†æçµæœ' å‰µå»ºæˆåŠŸã€‚\n",
            "å·²ç‚ºå·¥ä½œè¡¨ 'åˆ†æçµæœ' å¯«å…¥è¡¨é ­ã€‚\n",
            "ç¢ºä¿å·¥ä½œè¡¨ 'åˆ†æçµæœ' éç¨‹çµæŸã€‚\n",
            "æ‰€æœ‰å¿…è¦å·¥ä½œè¡¨ç¢ºä¿å®Œæˆã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a6a0ca1"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (å®šç¾©è³‡æ–™è¡¨çš„æ¬„ä½åç¨±å’Œåˆå§‹åŒ– DataFrame)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0835c55"
      },
      "source": [
        "# å®šç¾©ä¸åŒè³‡æ–™è¡¨çš„æ¬„ä½åç¨± (Header)\n",
        "# PTT æ–‡ç« è³‡æ–™çš„æ¬„ä½\n",
        "PTT_HEADER = [\n",
        "    \"post_id\", \"title\", \"url\", \"date\", \"author\", \"nrec\", \"created_at\",\n",
        "    \"fetched_at\", \"content\"\n",
        "]\n",
        "# è¼¸å…¥è³‡æ–™çš„æ¬„ä½ (æ‚¨éœ€è¦æ ¹æ“šæ‚¨çš„ Sheet èª¿æ•´)\n",
        "INPUT_HEADER = [\"Timestamp\", \"é–‹æ”¾å¼å›ç­”\"] # <-- è«‹æ ¹æ“šæ‚¨çš„ Sheet è¼¸å…¥å·¥ä½œè¡¨æ¨™é ­èª¿æ•´é€™è£¡çš„æ¬„ä½åç¨±\n",
        "# æ–‡æœ¬åˆ†æçµæœçš„æ¬„ä½\n",
        "ANALYSIS_HEADER = [\"term\", \"freq\", \"df_count\", \"tfidf_mean\", \"examples\"]\n",
        "\n",
        "# åˆå§‹åŒ–ç©ºçš„ DataFrameï¼Œå¾ŒçºŒç”± Gradio è®€å–æˆ–çˆ¬å–å¾Œå¡«å……\n",
        "ptt_posts_df = pd.DataFrame(columns=PTT_HEADER)\n",
        "input_responses_df = pd.DataFrame(columns=INPUT_HEADER)\n",
        "terms_df = pd.DataFrame(columns=ANALYSIS_HEADER)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddbef19"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (é€²è¡Œ Google èº«ä»½é©—è­‰)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1de3818f"
      },
      "source": [
        "# é€²è¡Œ Google èº«ä»½é©—è­‰\n",
        "# é€™æœƒåœ¨ Colab ç’°å¢ƒä¸­è·³å‡ºä¸€å€‹è¦–çª—ï¼Œå¼•å°æ‚¨å®Œæˆæˆæ¬Šæµç¨‹\n",
        "# æˆæ¬Šå¾Œï¼Œç¨‹å¼ç¢¼æ‰èƒ½å­˜å–æ‚¨çš„ Google Drive å’Œ Google Sheets\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# ä½¿ç”¨æ‚¨çš„ Google æ†‘è­‰ä¾†æˆæ¬Š gspread\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "# åˆå§‹åŒ– gspread å®¢æˆ¶ç«¯ï¼Œä¹‹å¾Œå°±å¯ä»¥ç”¨ gc ä¾†æ“ä½œæ‚¨çš„ Google Sheets\n",
        "gc = gspread.authorize(creds)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2eb938f"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (ç¢ºä¿ Google Sheet è©¦ç®—è¡¨å­˜åœ¨)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de441dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661a5f82-446c-4309-e798-bb6a5e78b943"
      },
      "source": [
        "# ç¢ºä¿æŒ‡å®šçš„ Google Sheet å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºä¸€å€‹æ–°çš„\n",
        "def ensure_spreadsheet(url):\n",
        "    try:\n",
        "        # Try to open the spreadsheet using the URL\n",
        "        sh = gc.open_by_url(url)\n",
        "        print(f\"âœ… Successfully opened spreadsheet from URL: {url}\")\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        print(f\"âš ï¸ Spreadsheet not found at URL: {url}. Attempting to create a new one (this might not be the intended behavior if you expected to use an existing sheet).\")\n",
        "        # If not found by URL, try creating one. Note: creating by URL is not standard gspread.\n",
        "        # A more typical approach is to create by name or rely on an existing sheet.\n",
        "        # For this scenario, let's assume the user wants to use the sheet at the URL.\n",
        "        # If it's not found, there might be an issue with the URL or permissions.\n",
        "        # We'll re-raise the error or return None to indicate failure.\n",
        "        raise # Re-raise the error to inform the user the sheet wasn't found\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed to open spreadsheet from URL {url}: {e}\")\n",
        "        raise # Re-raise any other exceptions\n",
        "\n",
        "    return sh\n",
        "\n",
        "# Use the provided SHEET_URL to ensure the spreadsheet exists and is opened\n",
        "try:\n",
        "    sh = ensure_spreadsheet(SHEET_URL)\n",
        "    print(f\"âœ… Google Sheet '{SHEET_URL}' ç¢ºä¿å®Œæˆï¼Œæº–å‚™æª¢æŸ¥å·¥ä½œè¡¨ã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ç„¡æ³•é–‹å•Ÿæˆ–å»ºç«‹ Google Sheet: {e}\")\n",
        "    # Depending on the severity, you might want to exit or disable parts of the app\n",
        "    sh = None # Set sh to None to prevent further errors if spreadsheet opening failed"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully opened spreadsheet from URL: https://docs.google.com/spreadsheets/d/163AOmd5HnH9sxzYJo4BwZeE87UFr3k5XxuggcRNsZCs/edit?usp=sharing\n",
            "âœ… Google Sheet 'https://docs.google.com/spreadsheets/d/163AOmd5HnH9sxzYJo4BwZeE87UFr3k5XxuggcRNsZCs/edit?usp=sharing' ç¢ºä¿å®Œæˆï¼Œæº–å‚™æª¢æŸ¥å·¥ä½œè¡¨ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7e3cc6d"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (ç¢ºä¿ Google Sheet å·¥ä½œè¡¨å­˜åœ¨ä¸¦å®šç¾©è¼”åŠ©å‡½å¼ read_df, write_df, tznow)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "314be9d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9f54d4-fb81-4415-d63d-4fb237af605f"
      },
      "source": [
        "# ç¢ºä¿æŒ‡å®šçš„å·¥ä½œè¡¨ (worksheet) åœ¨è©¦ç®—è¡¨ä¸­å­˜åœ¨ï¼Œä¸¦æª¢æŸ¥/è£œä¸Šè¡¨é ­\n",
        "def ensure_worksheet(sh, title, header):\n",
        "    if sh is None:\n",
        "        print(f\"âš ï¸ è©¦ç®—è¡¨ç‰©ä»¶ç‚º Noneï¼Œç„¡æ³•ç¢ºä¿å·¥ä½œè¡¨ '{title}'ã€‚è«‹æª¢æŸ¥è©¦ç®—è¡¨é–‹å•Ÿæ­¥é©Ÿã€‚\")\n",
        "        return None\n",
        "\n",
        "    print(f\"å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ '{title}' å­˜åœ¨æ–¼è©¦ç®—è¡¨ '{sh.title}'...\")\n",
        "    try:\n",
        "        ws = sh.worksheet(title) # å˜—è©¦é–‹å•Ÿç¾æœ‰çš„å·¥ä½œè¡¨\n",
        "        print(f\"å·¥ä½œè¡¨ '{title}' å·²å­˜åœ¨ã€‚\")\n",
        "    except gspread.WorksheetNotFound:\n",
        "        print(f\"å·¥ä½œè¡¨ '{title}' ä¸å­˜åœ¨æ–¼è©¦ç®—è¡¨ '{sh.title}'ï¼Œå˜—è©¦å‰µå»º...\")\n",
        "        try:\n",
        "            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå‰‡æ–°å¢ä¸€å€‹å·¥ä½œè¡¨ï¼Œä¸¦è¨­å®šåˆå§‹çš„è¡Œåˆ—æ•¸\n",
        "            ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "            print(f\"å·¥ä½œè¡¨ '{title}' å‰µå»ºæˆåŠŸã€‚\")\n",
        "            # ws.update([header]) # æ–°å¢æ™‚å¯«å…¥è¡¨é ­ - Moved below to handle existing sheets without headers too\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ å‰µå»ºå·¥ä½œè¡¨ '{title}' å¤±æ•—: {e}\")\n",
        "            # Depending on the severity, you might want to exit or disable parts of the app\n",
        "            return None # Return None if worksheet creation fails\n",
        "\n",
        "    # å†æ¬¡æª¢æŸ¥ï¼šè‹¥æ²’æœ‰è¡¨é ­ï¼ˆå·¥ä½œè¡¨æ˜¯ç©ºçš„æˆ–ç¬¬ä¸€è¡Œä¸æ˜¯é æœŸçš„è¡¨é ­ï¼‰ï¼Œå‰‡æ¸…ç©ºä¸¦å¯«ä¸Šè¡¨é ­\n",
        "    try:\n",
        "        data = ws.get_all_values('A1:1') # Only read the first row\n",
        "        if not data or data[0] != header:\n",
        "            print(f\"å·¥ä½œè¡¨ '{title}' æ²’æœ‰è¡¨é ­æˆ–è¡¨é ­ä¸ç¬¦ï¼Œæ¸…ç©ºä¸¦é‡æ–°å¯«å…¥è¡¨é ­ã€‚\")\n",
        "            ws.clear() # æ¸…ç©ºå·¥ä½œè¡¨å…§å®¹\n",
        "            ws.update([header]) # é‡æ–°å¯«å…¥è¡¨é ­\n",
        "            print(f\"å·²ç‚ºå·¥ä½œè¡¨ '{title}' é‡æ–°å¯«å…¥è¡¨é ­ã€‚\")\n",
        "        else:\n",
        "            print(f\"å·¥ä½œè¡¨ '{title}' è¡¨é ­æª¢æŸ¥æ­£å¸¸ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æª¢æŸ¥æˆ–æ›´æ–°å·¥ä½œè¡¨ '{title}' è¡¨é ­å¤±æ•—: {e}\")\n",
        "        # é€™è£¡é¸æ“‡ä¸ä¸­æ–·ç¨‹å¼ï¼Œä½†è¨˜éŒ„éŒ¯èª¤\n",
        "\n",
        "    print(f\"ç¢ºä¿å·¥ä½œè¡¨ '{title}' éç¨‹çµæŸã€‚\")\n",
        "    return ws\n",
        "\n",
        "# è®€å– DataFrame çš„è¼”åŠ©å‡½å¼\n",
        "def read_df(ws, header):\n",
        "    \"\"\"å¾ Google Sheet å·¥ä½œè¡¨è®€å–è³‡æ–™åˆ° DataFrameï¼Œä¸¦ç¢ºä¿æ¬„ä½å’Œå‹æ…‹æ­£ç¢º\"\"\"\n",
        "    if ws is None:\n",
        "        print(\"âš ï¸ å·¥ä½œè¡¨ç‰©ä»¶ç‚º Noneï¼Œç„¡æ³•è®€å– DataFrameã€‚\")\n",
        "        cols = header # Use provided header for empty DataFrame\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    try:\n",
        "        df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "        if df is None or df.empty:\n",
        "            print(f\"â„¹ï¸ å·¥ä½œè¡¨ '{ws.title}' ç‚ºç©ºæˆ–ç„¡æ³•è®€å– DataFrameã€‚\")\n",
        "            sheet_header = ws.get_all_values('A1:1')\n",
        "            cols = sheet_header[0] if sheet_header and sheet_header[0] else header\n",
        "            return pd.DataFrame(columns=cols)\n",
        "\n",
        "        df = df.fillna(\"\")\n",
        "        # Ensure all header columns exist, add if missing\n",
        "        for c in header:\n",
        "            if c not in df.columns:\n",
        "                print(f\"âš ï¸ å·¥ä½œè¡¨ '{ws.title}' ç¼ºå°‘æ¬„ä½ '{c}'ï¼Œå·²æ–°å¢ç©ºæ¬„ä½ã€‚\")\n",
        "                df[c] = \"\"\n",
        "        # Optional: Reorder columns to match header\n",
        "        df = df[header]\n",
        "\n",
        "        print(f\"âœ… å¾å·¥ä½œè¡¨ '{ws.title}' è®€å– {len(df)} ç­†è³‡æ–™ã€‚\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ å¾å·¥ä½œè¡¨ '{ws.title}' è®€å– DataFrame å¤±æ•—: {e}\")\n",
        "        cols = ws.get_all_values('A1:1')[0] if ws.get_all_values('A1:1') else header\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "\n",
        "# å°‡ DataFrame å¯«å…¥ Google Sheet çš„è¼”åŠ©å‡½å¼\n",
        "def write_df(ws, df, header):\n",
        "    \"\"\"å°‡ DataFrame å…§å®¹å¯«å…¥ Google Sheet å·¥ä½œè¡¨ (å…¨é‡è¦†è“‹)\"\"\"\n",
        "    if ws is None:\n",
        "        print(\"âš ï¸ å·¥ä½œè¡¨ç‰©ä»¶ç‚º Noneï¼Œç„¡æ³•å¯«å…¥ DataFrameã€‚\")\n",
        "        return\n",
        "\n",
        "    print(f\"å˜—è©¦å°‡ {len(df)} ç­†è³‡æ–™å¯«å…¥å·¥ä½œè¡¨ '{ws.title}'...\")\n",
        "    try:\n",
        "        if df.empty:\n",
        "            ws.clear()\n",
        "            ws.update([header])\n",
        "            print(f\"âœ… å·¥ä½œè¡¨ '{ws.title}' å·²æ¸…ç©ºä¸¦å¯«å…¥è¡¨é ­ã€‚\")\n",
        "            return\n",
        "\n",
        "        df_out = df.copy()\n",
        "        # Ensure only header columns are written and in the correct order\n",
        "        df_out = df_out.reindex(columns=header, fill_value=\"\")\n",
        "        for c in df_out.columns:\n",
        "            df_out[c] = df_out[c].astype(str)\n",
        "\n",
        "        ws.clear()\n",
        "        ws.update([header] + df_out[header].values.tolist())\n",
        "        print(f\"âœ… å·²å°‡ {len(df)} ç­†è³‡æ–™å¯«å…¥å·¥ä½œè¡¨ '{ws.title}'ã€‚\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ å°‡ DataFrame å¯«å…¥å·¥ä½œè¡¨ '{ws.title}' å¤±æ•—: {e}\")\n",
        "\n",
        "\n",
        "# éœ€è¦ tznow å‡½å¼ä¾†è™•ç†æ™‚é–“æˆ³è¨˜\n",
        "def tznow():\n",
        "    from datetime import datetime as dt\n",
        "    from dateutil.tz import gettz\n",
        "    # ç¢ºä¿ TIMEZONE è®Šæ•¸å·²å®šç¾©\n",
        "    try:\n",
        "        TIMEZONE\n",
        "    except NameError:\n",
        "        # å¦‚æœ TIMEZONE æœªå®šç¾©ï¼Œçµ¦å‡ºè­¦å‘Šä¸¦ä½¿ç”¨é è¨­å€¼\n",
        "        print(\"âš ï¸ æ™‚å€è®Šæ•¸ TIMEZONE æœªå®šç¾©ï¼Œè«‹åŸ·è¡Œè¨­å®šå„²å­˜æ ¼ã€‚é€™è£¡å°‡ä½¿ç”¨é è¨­å€¼ Asia/Taipeiã€‚\")\n",
        "        TIMEZONE_DEFAULT = \"Asia/Taipei\"\n",
        "        return dt.now(gettz(TIMEZONE_DEFAULT))\n",
        "    # å¦‚æœ TIMEZONE å·²å®šç¾©ï¼Œå‰‡ä½¿ç”¨å®ƒ\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "# ç¢ºä¿ PTT æ–‡ç« è³‡æ–™ã€è¼¸å…¥è³‡æ–™å’Œåˆ†æçµæœçš„å·¥ä½œè¡¨å­˜åœ¨ä¸¦æœ‰æ­£ç¢ºçš„è¡¨é ­\n",
        "# ä½¿ç”¨å‰é¢å®šç¾©çš„ ensure_worksheet å‡½å¼ä¾†è™•ç†\n",
        "# Ensure the spreadsheet object 'sh' is available before calling ensure_worksheet\n",
        "if 'sh' in globals() and sh is not None:\n",
        "    print(\"ç¢ºä¿ PTT æ–‡ç« å·¥ä½œè¡¨...\")\n",
        "    ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER) # PTT æ–‡ç« å·¥ä½œè¡¨\n",
        "    print(\"ç¢ºä¿ è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨...\")\n",
        "    ws_input = ensure_worksheet(sh, INPUT_WORKSHEET_NAME, INPUT_HEADER) # è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨\n",
        "    print(\"ç¢ºä¿ åˆ†æçµæœå·¥ä½œè¡¨...\")\n",
        "    ws_analysis = ensure_worksheet(sh, OUTPUT_WORKSHEET_NAME, ANALYSIS_HEADER) # åˆ†æçµæœå·¥ä½œè¡¨\n",
        "\n",
        "    print(\"æ‰€æœ‰å¿…è¦å·¥ä½œè¡¨ç¢ºä¿å®Œæˆã€‚\")\n",
        "else:\n",
        "    print(\"âš ï¸ è©¦ç®—è¡¨ç‰©ä»¶ 'sh' æœªæˆåŠŸåˆå§‹åŒ–ï¼Œè·³éå·¥ä½œè¡¨ç¢ºä¿æ­¥é©Ÿã€‚\")"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç¢ºä¿ PTT æ–‡ç« å·¥ä½œè¡¨...\n",
            "å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ 'ptt_movie_posts' å­˜åœ¨æ–¼è©¦ç®—è¡¨ 'hw4'...\n",
            "å·¥ä½œè¡¨ 'ptt_movie_posts' å·²å­˜åœ¨ã€‚\n",
            "å·¥ä½œè¡¨ 'ptt_movie_posts' è¡¨é ­æª¢æŸ¥æ­£å¸¸ã€‚\n",
            "ç¢ºä¿å·¥ä½œè¡¨ 'ptt_movie_posts' éç¨‹çµæŸã€‚\n",
            "ç¢ºä¿ è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨...\n",
            "å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' å­˜åœ¨æ–¼è©¦ç®—è¡¨ 'hw4'...\n",
            "å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' å·²å­˜åœ¨ã€‚\n",
            "å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' è¡¨é ­æª¢æŸ¥æ­£å¸¸ã€‚\n",
            "ç¢ºä¿å·¥ä½œè¡¨ 'å·¥ä½œè¡¨1' éç¨‹çµæŸã€‚\n",
            "ç¢ºä¿ åˆ†æçµæœå·¥ä½œè¡¨...\n",
            "å˜—è©¦ç¢ºä¿å·¥ä½œè¡¨ 'åˆ†æçµæœ' å­˜åœ¨æ–¼è©¦ç®—è¡¨ 'hw4'...\n",
            "å·¥ä½œè¡¨ 'åˆ†æçµæœ' å·²å­˜åœ¨ã€‚\n",
            "å·¥ä½œè¡¨ 'åˆ†æçµæœ' è¡¨é ­æª¢æŸ¥æ­£å¸¸ã€‚\n",
            "ç¢ºä¿å·¥ä½œè¡¨ 'åˆ†æçµæœ' éç¨‹çµæŸã€‚\n",
            "æ‰€æœ‰å¿…è¦å·¥ä½œè¡¨ç¢ºä¿å®Œæˆã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a74eddae"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (å®‰è£å¿…è¦çš„ Python å‡½å¼åº«)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef5eedee"
      },
      "source": [
        "# å®‰è£å¿…è¦çš„ Python å¥—ä»¶\n",
        "# é€™é‚Šåˆ—å‡ºäº†æˆ‘å€‘é€™å€‹å°ˆæ¡ˆæœƒç”¨åˆ°çš„æ‰€æœ‰å¥—ä»¶\n",
        "# gspread: ç”¨ä¾†æ“ä½œ Google Sheets\n",
        "# gspread_dataframe: æ–¹ä¾¿ Pandas DataFrame èˆ‡ Google Sheets ä¹‹é–“çš„è½‰æ›\n",
        "# google-auth, google-auth-oauthlib, google-auth-httplib2: Google èº«ä»½é©—è­‰ç›¸é—œ\n",
        "# gradio: å»ºç«‹ä¸€å€‹äº’å‹•å¼çš„ç¶²é ä»‹é¢\n",
        "# pandas: å¼·å¤§çš„è³‡æ–™è™•ç†å’Œåˆ†æå·¥å…·\n",
        "# beautifulsoup4: ç”¨æ–¼è§£æ HTML ç¶²é å…§å®¹ï¼Œæ–¹ä¾¿æŠ“å–è³‡æ–™\n",
        "# google-generativeai: å‘¼å« Google çš„ç”Ÿæˆå¼ AI æ¨¡å‹ (Gemini)\n",
        "# python-dateutil: è™•ç†æ—¥æœŸå’Œæ™‚é–“ï¼Œç‰¹åˆ¥æ˜¯æ™‚å€å•é¡Œ\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6df4c328"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (åŒ¯å…¥å°ˆæ¡ˆæ‰€éœ€çš„å‡½å¼åº«)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c91cca7"
      },
      "source": [
        "# åŒ¯å…¥å°ˆæ¡ˆæ‰€éœ€çš„å‡½å¼åº«\n",
        "# åŒ…å«äº†è™•ç†æ™‚é–“ã€è³‡æ–™ç§‘å­¸å·¥å…·ã€ç¶²é æ“ä½œã€æ–‡æœ¬åˆ†æä»¥åŠ Google æœå‹™çš„å‡½å¼åº«\n",
        "\n",
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz # è™•ç†æ™‚å€ç›¸é—œåŠŸèƒ½ï¼Œç¢ºä¿æ™‚é–“æ­£ç¢º\n",
        "\n",
        "import pandas as pd # Pandas DataFrame æ˜¯è³‡æ–™è™•ç†çš„æ ¸å¿ƒ\n",
        "import gradio as gr # Gradio è®“æˆ‘å€‘å¿«é€Ÿå»ºç«‹ä¸€å€‹åˆ†äº«çš„ä»‹é¢\n",
        "import requests # ç”¨æ–¼ç™¼é€ HTTP è«‹æ±‚ï¼Œä¾‹å¦‚å–å¾—ç¶²é åŸå§‹ç¢¼\n",
        "from bs4 import BeautifulSoup # BeautifulSoup å”åŠ©è§£æ HTML çµæ§‹\n",
        "\n",
        "# æ–‡æœ¬åˆ†æå¸¸ç”¨çš„å·¥å…·\n",
        "from collections import Counter, defaultdict # è¨ˆç®—è©é »ã€è™•ç†å­—å…¸\n",
        "import numpy as np # æ•¸å€¼è¨ˆç®—çš„å¥½å¹«æ‰‹\n",
        "from scipy.sparse import csr_matrix # å¦‚æœä½¿ç”¨åˆ° TF-IDF æˆ–å…¶ä»–å‘é‡åŒ–æ–¹æ³•æ™‚å¯èƒ½æœƒéœ€è¦\n",
        "\n",
        "# å‘¼å« Google çš„ç”Ÿæˆå¼ AI æ¨¡å‹ï¼Œä¾‹å¦‚ Gemini\n",
        "import google.generativeai as genai\n",
        "\n",
        "# èˆ‡ Google æœå‹™ (ç‰¹åˆ¥æ˜¯ Google Sheets) äº’å‹•çš„å‡½å¼åº«\n",
        "from google.colab import auth # åœ¨ Colab ç’°å¢ƒä¸­é€²è¡Œ Google èº«ä»½é©—è­‰\n",
        "import gspread # è®€å–å’Œå¯«å…¥ Google Sheets\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe # æ›´æ–¹ä¾¿åœ°è™•ç† DataFrame èˆ‡ Sheet ä¹‹é–“çš„è³‡æ–™å‚³è¼¸\n",
        "from google.auth.transport.requests import Request # è™•ç†é©—è­‰éç¨‹ä¸­çš„ HTTP è«‹æ±‚\n",
        "from google.oauth2 import service_account # æœå‹™å¸³æˆ¶é©—è­‰ (å¦‚æœæ‚¨ä½¿ç”¨æœå‹™å¸³æˆ¶é‡‘é‘°çš„è©±)\n",
        "from google.auth import default # ä½¿ç”¨é è¨­çš„èº«ä»½é©—è­‰æ†‘è­‰"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35381d80"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (è¨­å®š Gemini API é‡‘é‘°ä¸¦åˆå§‹åŒ–æ¨¡å‹)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d0e158b"
      },
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# å¾ Colab Secrets ä¸­ç²å– API é‡‘é‘°\n",
        "# è«‹å°‡æ‚¨çš„ Gemini API é‡‘é‘°å„²å­˜åœ¨ Colab çš„ Secrets ä¸­ï¼Œåç¨±è¨­å®šç‚º 'GOOGLE_API_KEY'\n",
        "# å¦‚ä½•è¨­å®š Secrets: åœ¨ Colab å·¦å´é¢æ¿é»æ“Šã€ŒğŸ”‘ã€ï¼Œæ–°å¢ä¸€å€‹ Secretï¼Œåç¨±ç‚º GOOGLE_API_KEYï¼Œå€¼è²¼ä¸Šæ‚¨çš„ API é‡‘é‘°\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# ä½¿ç”¨ç²å–çš„é‡‘é‘°é…ç½® genai\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# åˆå§‹åŒ– Gemini æ¨¡å‹ï¼Œé€™è£¡ä½¿ç”¨ 'gemini-2.5-pro'\n",
        "# æ‚¨å¯ä»¥æ ¹æ“šéœ€æ±‚æ›´æ›æ¨¡å‹åç¨±\n",
        "model = genai.GenerativeModel('gemini-2.5-pro')"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415e277f"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (å®šç¾©æ ¸å¿ƒåŠŸèƒ½å‡½å¼ï¼šPTT çˆ¬èŸ²ã€Sheet è®€å–ã€æ–‡æœ¬åˆ†æã€AI ç”Ÿæˆ)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cc5bdc3"
      },
      "source": [
        "# ==============\n",
        "# PTT é›»å½±ç‰ˆçˆ¬èŸ²å‡½å¼\n",
        "# ==============\n",
        "\n",
        "# (ä¿ç•™åŸæœ‰çš„ PTT çˆ¬èŸ²è¼”åŠ©å‡½å¼ _get_soup, _get_prev_index_url, _parse_nrec, _extract_post_list, _clean_ptt_content)\n",
        "PTT_MOVIE_INDEX = \"https://www.ptt.cc/bbs/movie/index.html\" # PTT é›»å½±ç‰ˆé¦–é  URL\n",
        "PTT_COOKIES = {\"over18\": \"1\"} # ç”¨æ–¼ç¹é PTT çš„æ»¿ 18 æ­²é©—è­‰ (æœƒè©±ç‹€æ…‹ç¶­æŒ)\n",
        "\n",
        "def _get_soup(url):\n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "    r = requests.get(url, timeout=15, headers=headers, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    for a in btns:\n",
        "        if \"ä¸Šé \" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                from urllib.parse import urljoin\n",
        "                return urljoin(PTT_MOVIE_INDEX, href)\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    if not nrec_span: return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"çˆ†\": return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a: continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec})\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    for p in soup.select(\"div.push\"): p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main: return \"\", \"\"\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas: m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text: text = text.split(\"--\")[0].strip()\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "\n",
        "# ä¸»è¦çˆ¬èŸ²å‡½å¼ï¼šå¾ PTT é›»å½±ç‰ˆæŠ“å–æ–‡ç« \n",
        "# æŠ“å–çµæœæœƒå¯«å…¥ ws_ptt_posts å·¥ä½œè¡¨\n",
        "def crawl_ptt_movie(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"å¾æœ€æ–° index.html å¾€å‰ç¿» index_pages é ï¼ŒæŠ“æ»¿è¶³æ¢ä»¶çš„æ–‡ç« ä¸¦å¯«å…¥ Sheet\"\"\"\n",
        "    global ptt_posts_df, ws_ptt_posts # å®£å‘Šä½¿ç”¨å…¨åŸŸè®Šæ•¸\n",
        "    url = PTT_MOVIE_INDEX\n",
        "    all_rows = []\n",
        "    # å…ˆå¾ Sheet è®€å–ç¾æœ‰çš„ PTT æ–‡ç« ï¼Œç”¨æ–¼å»é‡\n",
        "    ptt_posts_df = read_df(ws_ptt_posts, PTT_HEADER) # å¾ PTT å·¥ä½œè¡¨è®€å–\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ æŠ“å–é é¢å¤±æ•— {url}: {e}\")\n",
        "            break\n",
        "        posts = _extract_post_list(soup)\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push): continue\n",
        "            if keyword and (keyword not in p[\"title\"]): continue\n",
        "            if p[\"url\"] in seen_urls: continue\n",
        "\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ æŠ“å–æ–‡ç« å…§æ–‡å¤±æ•— {p['url']}: {e}\")\n",
        "                content, meta_title = \"\", \"\"\n",
        "\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"ï¼ˆç„¡æ¨™é¡Œï¼‰\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(), # éœ€è¦ tznow å‡½å¼\n",
        "                \"fetched_at\": tznow().isoformat(), # éœ€è¦ tznow å‡½å¼\n",
        "                \"content\": content\n",
        "            })\n",
        "            seen_urls.add(p[\"url\"])\n",
        "\n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev: break\n",
        "        url = prev\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        # åˆä½µæ–°æŠ“å–çš„è³‡æ–™åˆ°ç¾æœ‰ DataFrame\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        # å°‡æ›´æ–°å¾Œçš„ DataFrame å¯«å› PTT å·¥ä½œè¡¨\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        return f\"âœ… å–å¾— {len(all_rows)} ç¯‡æ–‡ç« ï¼ˆå·²å¯«å…¥ Sheet çš„ ptt_movie_posts å·¥ä½œè¡¨ï¼‰\", ptt_posts_df\n",
        "    else:\n",
        "        return \"â„¹ï¸ æ²’æœ‰æ–°æ–‡ç« ç¬¦åˆæ¢ä»¶ï¼ˆæˆ–å…§å®¹å·²åœ¨ Sheet çš„ ptt_movie_posts å·¥ä½œè¡¨ï¼‰\", ptt_posts_df\n",
        "\n",
        "\n",
        "# ==============\n",
        "# æ–‡æœ¬åˆ†æï¼ˆjieba + TF/IDF + bigramï¼‰å‡½å¼\n",
        "# ==============\n",
        "import re\n",
        "try:\n",
        "    import jieba\n",
        "except ImportError:\n",
        "    jieba = None\n",
        "    print(\"âš ï¸ æœªå®‰è£ jiebaï¼Œä¸­æ–‡æ–·è©å°‡ä½¿ç”¨è¼ƒç°¡å–®çš„ç©ºç™½å­—å…ƒåˆ†å‰²ã€‚è«‹è€ƒæ…®å®‰è£ jieba ä»¥ç²å¾—æ›´å¥½çš„æ–·è©æ•ˆæœã€‚\")\n",
        "\n",
        "def _tokenize_zh(text):\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9,.!?:;\\'\\\"()]+\", \" \", text)\n",
        "    if not jieba:\n",
        "        return [t.strip() for t in text.split() if len(t.strip()) > 1]\n",
        "    tokens = [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "    return tokens\n",
        "\n",
        "# å¾ Google Sheet è®€å–é–‹æ”¾å¼å›ç­”è³‡æ–™çš„å‡½å¼ (ä¿ç•™)\n",
        "def read_open_ended_responses(worksheet_name=INPUT_WORKSHEET_NAME, column_name=\"é–‹æ”¾å¼å›ç­”\"):\n",
        "    \"\"\"å¾æŒ‡å®šçš„ Google Sheet å·¥ä½œè¡¨å’Œæ¬„ä½è®€å–æ–‡å­—è³‡æ–™\"\"\"\n",
        "    global sh, ws_input # ä½¿ç”¨å‰é¢å·²ç¶“æˆæ¬Šä¸¦é–‹å•Ÿçš„è©¦ç®—è¡¨ç‰©ä»¶å’Œè¼¸å…¥å·¥ä½œè¡¨ç‰©ä»¶\n",
        "    try:\n",
        "        # å˜—è©¦é–‹å•ŸæŒ‡å®šçš„å·¥ä½œè¡¨ï¼Œå¦‚æœæŒ‡å®šçš„æ˜¯ INPUT_WORKSHEET_NAMEï¼Œå‰‡ç›´æ¥ä½¿ç”¨ ws_input\n",
        "        ws = sh.worksheet(worksheet_name) if worksheet_name != INPUT_WORKSHEET_NAME else ws_input\n",
        "        df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "        if df is None or df.empty:\n",
        "            return pd.DataFrame(columns=[column_name]), f\"â„¹ï¸ å·¥ä½œè¡¨ '{worksheet_name}' æ²’æœ‰è³‡æ–™ã€‚\"\n",
        "\n",
        "        if column_name not in df.columns:\n",
        "             return pd.DataFrame(columns=[column_name]), f\"âš ï¸ å·¥ä½œè¡¨ '{worksheet_name}' ä¸­æ‰¾ä¸åˆ° '{column_name}' æ¬„ä½ã€‚\"\n",
        "        df = df.fillna(\"\")\n",
        "        return df[[column_name]], f\"âœ… å¾å·¥ä½œè¡¨ '{worksheet_name}' è®€å– {len(df)} ç­†è³‡æ–™ã€‚\"\n",
        "\n",
        "    except gspread.WorksheetNotFound:\n",
        "        return pd.DataFrame(columns=[column_name]), f\"âš ï¸ æ‰¾ä¸åˆ°å·¥ä½œè¡¨ '{worksheet_name}'ã€‚\"\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(columns=[column_name]), f\"âš ï¸ è®€å–å·¥ä½œè¡¨å¤±æ•—: {e}\"\n",
        "\n",
        "\n",
        "# ä¸»è¦æ–‡æœ¬åˆ†æå‡½å¼ - ä¿®æ”¹ç‚ºæ¥å— DataFrame ä½œç‚ºè¼¸å…¥\n",
        "def analyze_texts(df, text_column_name, topk=50, min_df=2):\n",
        "    \"\"\"å¾ DataFrame ä¸­çš„æŒ‡å®šæ–‡å­—æ¬„ä½é€²è¡Œæ–‡æœ¬åˆ†æï¼Œçµæœå¯«å…¥åˆ†æå·¥ä½œè¡¨\"\"\"\n",
        "    global ws_analysis # ä½¿ç”¨å‰é¢ç¢ºä¿å­˜åœ¨çš„åˆ†æçµæœå·¥ä½œè¡¨\n",
        "    if df is None or df.empty or text_column_name not in df.columns:\n",
        "        return \"ğŸ“­ æ²’æœ‰æœ‰æ•ˆçš„æ–‡å­—è³‡æ–™é€²è¡Œåˆ†æã€‚\", pd.DataFrame(columns=ANALYSIS_HEADER), \"\"\n",
        "\n",
        "    docs = df[text_column_name].tolist()\n",
        "    docs = [d for d in docs if d and isinstance(d, str)]\n",
        "\n",
        "    if not docs:\n",
        "         return \"ğŸ“­ æ²’æœ‰æœ‰æ•ˆçš„æ–‡å­—è³‡æ–™é€²è¡Œåˆ†æã€‚\", pd.DataFrame(columns=ANALYSIS_HEADER), \"\"\n",
        "\n",
        "    # (ä¿ç•™åŸæœ‰çš„è©é »ã€æ–‡ä»¶é »ç‡ã€TF-IDFã€Bigram è¨ˆç®—é‚è¼¯)\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "    token_docs = []\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc)\n",
        "        token_docs.append(toks)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    filtered_terms = [t for t in freq.keys() if df_cnt[t] >= int(min_df)]\n",
        "    freq = Counter({t: freq[t] for t in filtered_terms})\n",
        "    df_cnt = {t: df_cnt[t] for t in filtered_terms}\n",
        "\n",
        "    tfidf_map = {}\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False)\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        term_indices = [i for i, t in enumerate(terms) if t in filtered_terms]\n",
        "        X = X[:, term_indices]\n",
        "        terms = [terms[i] for i in term_indices]\n",
        "        tfidf_mean = X.mean(axis=0).A1 if X.shape[0] > 0 and X.shape[1] > 0 else np.array([])\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except ImportError:\n",
        "         print(\"âš ï¸ æœªå®‰è£ scikit-learnï¼Œç„¡æ³•è¨ˆç®— TF-IDFã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ TF-IDF è¨ˆç®—å¤±æ•—: {e}\")\n",
        "\n",
        "    from itertools import tee\n",
        "    def pairwise(iterable):\n",
        "        \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
        "        a, b = tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "    bigram_freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        bigram_freq.update([\" \".join(bg) for bg in pairwise(toks)])\n",
        "\n",
        "    candidates = list(freq.keys())\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t,0.0) if tfidf_map else 0.0, 6), freq.get(t,0)), reverse=True)\n",
        "    top_terms = candidates[:int(topk)]\n",
        "\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq.get(t,0)),\n",
        "            \"df_count\": str(df_cnt.get(t,0)),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0) if tfidf_map else 0.0:.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "    terms_df = pd.DataFrame(rows, columns=ANALYSIS_HEADER)\n",
        "\n",
        "    # å°‡çµæœå¯«å› Google Sheet çš„åˆ†æçµæœå·¥ä½œè¡¨\n",
        "    write_df(ws_analysis, terms_df, ANALYSIS_HEADER)\n",
        "\n",
        "    # ç”¢ç”Ÿ Markdown æ ¼å¼çš„æ‘˜è¦å ±å‘Š\n",
        "    md_lines = []\n",
        "    md_lines.append(f\"### é—œéµè© Top {len(top_terms)}ï¼ˆä¾ TF-IDF å¹³å‡å€¼å„ªå…ˆï¼Œæ¬¡åºå†ä»¥è©é »ï¼‰\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        tfidf_val = float(tfidf_map.get(t,0.0) if tfidf_map else 0.0)\n",
        "        md_lines.append(f\"{i}. **{t}** â€” tfidfâ‰ˆ{tfidf_val:.4f}ï¼›freq={freq.get(t,0)}ï¼›df={df_cnt.get(t,0)}\")\n",
        "    md_lines.append(\"\\n### å¸¸è¦‹é›™è©æ­é…ï¼ˆå‰ 20ï¼‰\")\n",
        "    for i, (bg, c) in enumerate(bigram_freq.most_common(20), 1):\n",
        "        md_lines.append(f\"{i}. {bg} â€” {c}\")\n",
        "\n",
        "    return f\"âœ… å·²å®Œæˆæ–‡æœ¬åˆ†æï¼Œå…±è™•ç† {len(docs)} ç¯‡æ–‡æª”ï¼›åˆ†æçµæœå·²å¯«å…¥ Sheet çš„ '{OUTPUT_WORKSHEET_NAME}' å·¥ä½œè¡¨ã€‚\", terms_df, \"\\n\".join(md_lines)\n",
        "\n",
        "# è®€å– DataFrame çš„è¼”åŠ©å‡½å¼ (å¾ä¹‹å‰çš„ PTT çˆ¬èŸ²ç¨‹å¼ç¢¼æ¬ç§»éä¾†ä¸¦ä¿®æ”¹)\n",
        "def read_df(ws, header):\n",
        "    \"\"\"å¾ Google Sheet å·¥ä½œè¡¨è®€å–è³‡æ–™åˆ° DataFrameï¼Œä¸¦ç¢ºä¿æ¬„ä½å’Œå‹æ…‹æ­£ç¢º\"\"\"\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        sheet_header = ws.get_all_values('A1:1')\n",
        "        cols = sheet_header[0] if sheet_header and sheet_header[0] else header\n",
        "        return pd.DataFrame(columns=cols)\n",
        "    df = df.fillna(\"\")\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    return df\n",
        "\n",
        "# å°‡ DataFrame å¯«å…¥ Google Sheet çš„è¼”åŠ©å‡½å¼ (å¾ä¹‹å‰çš„ PTT çˆ¬èŸ²ç¨‹å¼ç¢¼æ¬ç§»éä¾†)\n",
        "def write_df(ws, df, header):\n",
        "    \"\"\"å°‡ DataFrame å…§å®¹å¯«å…¥ Google Sheet å·¥ä½œè¡¨ (å…¨é‡è¦†è“‹)\"\"\"\n",
        "    if df.empty:\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "        return\n",
        "    df_out = df.copy()\n",
        "    for c in df_out.columns:\n",
        "        df_out[c] = df_out[c].astype(str)\n",
        "    ws.clear()\n",
        "    ws.update([header] + df_out[header].values.tolist())\n",
        "\n",
        "# AI ç”Ÿæˆæ´å¯Ÿå’Œçµè«–çš„å‡½å¼ (ä¿ç•™)\n",
        "def generate_ai_output(terms_df):\n",
        "    \"\"\"ä½¿ç”¨ Gemini æ¨¡å‹ç”Ÿæˆæ´å¯Ÿæ‘˜è¦å’Œçµè«–\"\"\"\n",
        "    # global model # æ¨¡å‹å·²åœ¨å‰é¢å„²å­˜æ ¼åˆå§‹åŒ–\n",
        "    if terms_df is None or terms_df.empty:\n",
        "        return \"ğŸ“­ æ²’æœ‰åˆ†æçµæœï¼Œç„¡æ³•ç”Ÿæˆ AI æ‘˜è¦å’Œçµè«–ã€‚\", \"\", \"\"\n",
        "\n",
        "    input_text = \"è«‹æ ¹æ“šä»¥ä¸‹æ–‡æœ¬åˆ†æçµæœï¼Œç”Ÿæˆ 5 å¥æ´å¯Ÿæ‘˜è¦å’Œä¸€æ®µç´„ 120 å­—çš„çµè«–ï¼š\\n\\n\"\n",
        "    input_text += \"é—œéµè©åˆ—è¡¨ (ä¾é‡è¦æ€§æ’åº)ï¼š\\n\"\n",
        "    for _, r in terms_df.head(20).iterrows():\n",
        "        input_text += f\"- è©å½™: {r['term']}, é »ç‡: {r['freq']}, æ–‡ä»¶æ•¸: {r['df_count']}, TF-IDF: {float(r['tfidf_mean']):.4f}, ç¯„ä¾‹: {r['examples']}\\n\"\n",
        "\n",
        "    input_text += \"\\nè«‹æ³¨æ„ï¼š\\n1. æ´å¯Ÿæ‘˜è¦è«‹ä½¿ç”¨æ¢åˆ—å¼ã€‚\\n2. çµè«–è«‹é€£è²«æˆä¸€æ®µæ–‡å­—ï¼Œç´„ 120 å­—ã€‚\\n3. è«‹ç”¨ç¹é«”ä¸­æ–‡ã€‚\\n\\nç¯„ä¾‹è¼¸å‡ºæ ¼å¼ï¼š\\n### æ´å¯Ÿæ‘˜è¦\\n- æ´å¯Ÿ 1\\n- æ´å¯Ÿ 2\\n...\\n### çµè«–\\né€™è£¡æ˜¯ä¸€æ®µç´„ 120 å­—çš„çµè«–ã€‚\"\n",
        "\n",
        "    try:\n",
        "        # model = genai.GenerativeModel('gemini-2.5-pro') # æ¨¡å‹å·²åœ¨å‰é¢å„²å­˜æ ¼åˆå§‹åŒ–\n",
        "        resp = model.generate_content(input_text)\n",
        "        ai_output = resp.text\n",
        "\n",
        "        insights = []\n",
        "        conclusion = \"\"\n",
        "        if \"### æ´å¯Ÿæ‘˜è¦\" in ai_output and \"### çµè«–\" in ai_output:\n",
        "            parts = ai_output.split(\"### æ´å¯Ÿæ‘˜è¦\")\n",
        "            if len(parts) > 1:\n",
        "                insights_part_conc_part = parts[1].split(\"### çµè«–\")\n",
        "                if len(insights_part_conc_part) > 1:\n",
        "                    insights_text = insights_part_conc_part[0].strip()\n",
        "                    insights = [line.strip(\"- \").strip() for line in insights_text.split(\"\\n\") if line.strip().startswith(\"-\")]\n",
        "                    conclusion = insights_part_conc_part[1].strip()\n",
        "                else:\n",
        "                    insights_text = insights_part_conc_part[0].strip()\n",
        "                    insights = [line.strip(\"- \").strip() for line in insights_text.split(\"\\n\") if line.strip().startswith(\"-\")]\n",
        "            elif \"### çµè«–\" in ai_output:\n",
        "                 parts = ai_output.split(\"### çµè«–\")\n",
        "                 if len(parts) > 1:\n",
        "                     conclusion = parts[1].strip()\n",
        "        else:\n",
        "             insights = [ai_output]\n",
        "             conclusion = \"ï¼ˆç„¡æ³•è‡ªå‹•è§£æçµè«–ï¼‰\"\n",
        "\n",
        "        insights_md = \"### æ´å¯Ÿæ‘˜è¦\\n\" + (\"\\n\".join([f\"- {i}\" for i in insights]) if insights else \"ï¼ˆç„¡ï¼‰\")\n",
        "        conclusion_md = \"### çµè«–\\n\" + (conclusion if conclusion else \"ï¼ˆç„¡ï¼‰\")\n",
        "\n",
        "        return \"âœ… å·²æˆåŠŸç”Ÿæˆ AI æ‘˜è¦å’Œçµè«–ã€‚\", insights_md, conclusion_md\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ ç”Ÿæˆ AI æ‘˜è¦å’Œçµè«–å¤±æ•—: {e}\", \"### æ´å¯Ÿæ‘˜è¦\\nï¼ˆç”Ÿæˆå¤±æ•—ï¼‰\", \"### çµè«–\\nï¼ˆç”Ÿæˆå¤±æ•—ï¼‰\"\n",
        "\n",
        "# ç¢ºä¿ TIMEZONE è®Šæ•¸å·²å®šç¾©ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é è¨­å€¼ Asia/Taipei\n",
        "try:\n",
        "    TIMEZONE\n",
        "except NameError:\n",
        "    print(\"âš ï¸ æ™‚å€è®Šæ•¸ TIMEZONE æœªå®šç¾©ï¼Œé€™è£¡å°‡ä½¿ç”¨é è¨­å€¼ Asia/Taipeiã€‚\")\n",
        "    TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "# éœ€è¦ tznow å‡½å¼æ‰èƒ½è®“ crawl_ptt_movie å’Œå…¶ä»–éœ€è¦æ™‚é–“æˆ³çš„å‡½å¼é‹ä½œ\n",
        "def tznow():\n",
        "    from datetime import datetime as dt\n",
        "    from dateutil.tz import gettz\n",
        "    # ç¢ºä¿ TIMEZONE è®Šæ•¸å·²å®šç¾© (åœ¨æ­¤å„²å­˜æ ¼é–‹é ­å·²è™•ç†)\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "# åˆå§‹åŒ–ç©ºçš„ DataFrameï¼Œå¾ŒçºŒç”± Gradio è®€å–æˆ–çˆ¬å–å¾Œå¡«å……\n",
        "ptt_posts_df = pd.DataFrame(columns=PTT_HEADER)\n",
        "input_responses_df = pd.DataFrame(columns=INPUT_HEADER)\n",
        "terms_df = pd.DataFrame(columns=ANALYSIS_HEADER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31ed3329"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (å®šç¾© Gradio ä»‹é¢éœ€è¦å‘¼å«çš„è¼”åŠ©å‡½å¼)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ac9d8f2"
      },
      "source": [
        "# ==============\n",
        "# è¼”åŠ©å‡½å¼ï¼šè³‡æ–™è™•ç†èˆ‡åˆ†ææµç¨‹\n",
        "# å°‡éƒ¨åˆ†éœ€è¦åœ¨ Gradio ä»‹é¢ä¸­å‘¼å«çš„å‡½å¼ç§»åˆ° Gradio è¨­å®šä¹‹å‰å®šç¾©\n",
        "# ==============\n",
        "\n",
        "# è¼”åŠ©å‡½å¼ï¼šé‡æ–°æ•´ç† PTT æ–‡ç« è³‡æ–™ (å¾ Sheet è®€å–)\n",
        "def refresh_ptt_posts():\n",
        "    global ptt_posts_df, ws_ptt_posts, PTT_HEADER # ç¢ºä¿ä½¿ç”¨å…¨åŸŸè®Šæ•¸å’Œå·¥ä½œè¡¨ç‰©ä»¶\n",
        "    try:\n",
        "        # ç¢ºä¿ ws_ptt_posts å·²è¢«åˆå§‹åŒ– (å¦‚æœå‰é¢çš„å„²å­˜æ ¼åŸ·è¡Œäº†)\n",
        "        if 'ws_ptt_posts' not in globals():\n",
        "             return pd.DataFrame(columns=PTT_HEADER), \"âš ï¸ PTT æ–‡ç« å·¥ä½œè¡¨ç‰©ä»¶æœªåˆå§‹åŒ–ï¼Œè«‹åŸ·è¡Œå‰é¢ç›¸é—œå„²å­˜æ ¼ã€‚\"\n",
        "\n",
        "        ptt_posts_df = read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "        return ptt_posts_df, f\"âœ… å·²å¾ Sheet è¼‰å…¥ {len(ptt_posts_df)} ç¯‡ PTT æ–‡ç« ã€‚\"\n",
        "    except NameError:\n",
        "         return pd.DataFrame(columns=PTT_HEADER), \"âš ï¸ PTT_HEADER æˆ–ç›¸é—œç‰©ä»¶æœªå®šç¾©ï¼Œè«‹åŸ·è¡Œå‰é¢ç›¸é—œå„²å­˜æ ¼ã€‚\"\n",
        "\n",
        "\n",
        "# è¼”åŠ©å‡½å¼ï¼šé‡æ–°æ•´ç†é–‹æ”¾å¼å›ç­”è³‡æ–™ (å¾ Sheet è®€å–)\n",
        "def refresh_input_responses(worksheet_name, column_name):\n",
        "    global input_responses_df, sh, ws_input, INPUT_HEADER # ç¢ºä¿ä½¿ç”¨å…¨åŸŸè®Šæ•¸å’Œå·¥ä½œè¡¨ç‰©ä»¶\n",
        "    try:\n",
        "        # ç¢ºä¿ sh å’Œ ws_input å·²è¢«åˆå§‹åŒ– (å¦‚æœå‰é¢çš„å„²å­˜æ ¼åŸ·è¡Œäº†)\n",
        "        if 'sh' not in globals() or 'ws_input' not in globals():\n",
        "            return pd.DataFrame(columns=INPUT_HEADER), \"âš ï¸ Google Sheet ç‰©ä»¶æˆ–è¼¸å…¥è³‡æ–™å·¥ä½œè¡¨ç‰©ä»¶æœªåˆå§‹åŒ–ï¼Œè«‹åŸ·è¡Œå‰é¢ç›¸é—œå„²å­˜æ ¼ã€‚\"\n",
        "\n",
        "        # å‘¼å« oxbeeai_ni9s ä¸­å®šç¾©çš„ read_open_ended_responses å‡½å¼\n",
        "        input_responses_df, msg = read_open_ended_responses(worksheet_name, column_name)\n",
        "        return input_responses_df, msg\n",
        "    except NameError:\n",
        "        return pd.DataFrame(columns=INPUT_HEADER), \"âš ï¸ read_open_ended_responses æˆ–ç›¸é—œç‰©ä»¶æœªå®šç¾©ï¼Œè«‹åŸ·è¡Œå‰é¢ç›¸é—œå„²å­˜æ ¼ã€‚\"\n",
        "\n",
        "\n",
        "# è¼”åŠ©å‡½å¼ï¼šåŸ·è¡Œ PTT çˆ¬èŸ²ä¸¦æ›´æ–° DataFrame\n",
        "def run_ptt_crawl(index_pages, min_push, keyword):\n",
        "    global ptt_posts_df, ws_ptt_posts # ç¢ºä¿ä½¿ç”¨å…¨åŸŸè®Šæ•¸å’Œå·¥ä½œè¡¨ç‰©ä»¶\n",
        "    try:\n",
        "        # ç¢ºä¿ ws_ptt_posts å·²è¢«åˆå§‹åŒ–\n",
        "        if 'ws_ptt_posts' not in globals():\n",
        "            return pd.DataFrame(columns=PTT_HEADER), \"âš ï¸ PTT æ–‡ç« å·¥ä½œè¡¨ç‰©ä»¶æœªåˆå§‹åŒ–ï¼Œè«‹åŸ·è¡Œå‰é¢ç›¸é—œå„²å­˜æ ¼ã€‚\"\n",
        "\n",
        "        # å‘¼å« oxbeeai_ni9s ä¸­å®šç¾©çš„ crawl_ptt_movie å‡½å¼\n",
        "        msg, ptt_posts_df = crawl_ptt_movie(index_pages, min_push, keyword)\n",
        "        # crawl_ptt_movie å…§éƒ¨å·²ç¶“å°‡è³‡æ–™å¯«å› Sheet äº†\n",
        "        return msg, ptt_posts_df\n",
        "    except NameError:\n",
        "        return pd.DataFrame(columns=PTT_HEADER), \"âš ï¸ crawl_ptt_movie æˆ–ç›¸é—œç‰©ä»¶æœªå®šç¾©ï¼Œè«‹åŸ·è¡Œå‰é¢ç›¸é—œå„²å­˜æ ¼ã€‚\"\n",
        "\n",
        "\n",
        "# è¼”åŠ©å‡½å¼ï¼šåŸ·è¡Œæ–‡æœ¬åˆ†æå’Œ AI ç”Ÿæˆ\n",
        "def perform_analysis_and_ai(data_source, input_col, topk, min_df):\n",
        "    global ptt_posts_df, input_responses_df, terms_df # ç¢ºä¿ä½¿ç”¨å…¨åŸŸè®Šæ•¸\n",
        "    df_to_analyze = pd.DataFrame()\n",
        "    text_column = \"\"\n",
        "\n",
        "    if data_source == \"PTT æ–‡ç«  (å·²çˆ¬å–)\":\n",
        "        df_to_analyze = ptt_posts_df\n",
        "        # åˆ†æ PTT æ–‡ç« æ™‚ï¼Œåˆä½µ title å’Œ content ä½œç‚ºåˆ†ææ–‡æœ¬\n",
        "        df_to_analyze[\"combined_text\"] = df_to_analyze[\"title\"].fillna(\"\") + \"\\n\" + df_to_analyze[\"content\"].fillna(\"\")\n",
        "        text_column = \"combined_text\"\n",
        "    elif data_source == \"é–‹æ”¾å¼å›ç­” (å¾ Sheet è®€å–)\":\n",
        "        df_to_analyze = input_responses_df\n",
        "        text_column = input_col # ä½¿ç”¨ä½¿ç”¨è€…æŒ‡å®šçš„æ¬„ä½åç¨±\n",
        "        if text_column not in df_to_analyze.columns:\n",
        "             return \"âš ï¸ é¸æ“‡çš„è³‡æ–™ä¾†æºæˆ–æ¬„ä½åç¨±æœ‰èª¤ã€‚\", pd.DataFrame(columns=ANALYSIS_HEADER), \"\", \"\", \"\" # é¡å¤–è¿”å›ç©ºå­—ä¸²çµ¦ AI è¼¸å‡ºå€åŸŸ\n",
        "\n",
        "    if df_to_analyze.empty:\n",
        "        return \"ğŸ“­ æ²’æœ‰è³‡æ–™å¯ä»¥åˆ†æã€‚\", pd.DataFrame(columns=ANALYSIS_HEADER), \"\", \"\", \"\"\n",
        "\n",
        "    # å‘¼å« oxbeeai_ni9s ä¸­å®šç¾©çš„ analyze_texts å‡½å¼é€²è¡Œåˆ†æ\n",
        "    msg, terms_df, md_report = analyze_texts(df_to_analyze, text_column, topk, min_df)\n",
        "\n",
        "    # å‘¼å« oxbeeai_ni9s ä¸­å®šç¾©çš„ generate_ai_output å‡½å¼ç”Ÿæˆ AI æ´å¯Ÿå’Œçµè«–\n",
        "    ai_msg, insights_md, conclusion_md = generate_ai_output(terms_df)\n",
        "\n",
        "    # è¿”å›åˆ†æçµæœè¨Šæ¯ã€åˆ†æçµæœ DataFrameã€Markdown å ±å‘Šã€AI æ´å¯Ÿå’Œçµè«–\n",
        "    return msg, terms_df, md_report, insights_md, conclusion_md"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caba7a29"
      },
      "source": [
        "è«‹ä¿ç•™ä¸‹é¢é€™å€‹å„²å­˜æ ¼ (è¨­å®šä¸¦å•Ÿå‹• Gradio äº’å‹•å¼ä»‹é¢)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "bcca8037",
        "outputId": "d0946c06-5e26-498f-8f3c-1674b89cce37"
      },
      "source": [
        "# ==============\n",
        "# Gradio ä»‹é¢\n",
        "# é€™æ˜¯æ•´å€‹æ‡‰ç”¨ç¨‹å¼çš„ç¶²é ä»‹é¢éƒ¨åˆ†ï¼Œä½¿ç”¨ Gradio å‡½å¼åº«å»ºç«‹ã€‚\n",
        "# å®ƒåŒ…å«äº†ä¸åŒçš„åˆ†é ï¼Œç”¨æ–¼åŸ·è¡Œçˆ¬èŸ²ã€è®€å– Sheet è³‡æ–™ã€æ–‡æœ¬åˆ†æå’Œ AI ç”Ÿæˆæ‘˜è¦ã€‚\n",
        "# ==============\n",
        "\n",
        "# ä½¿ç”¨ gr.Blocks å»ºç«‹ä»‹é¢\n",
        "with gr.Blocks(title=\"æ–‡æœ¬åˆ†æèˆ‡ AI æ‘˜è¦å·¥å…·\") as demo:\n",
        "    gr.Markdown(\"# ğŸ“ æ–‡å­—è³‡æ–™åˆ†æèˆ‡ AI æ‘˜è¦å·¥å…·\") # ä¸»æ¨™é¡Œ\n",
        "\n",
        "    # --- è³‡æ–™ç²å–åˆ†é  ---\n",
        "    with gr.Tab(\"è³‡æ–™ç²å– (PTT & Sheet)\"):\n",
        "        gr.Markdown(\"### å¾ PTT é›»å½±ç‰ˆçˆ¬å–æ–‡ç« \")\n",
        "        with gr.Row():\n",
        "            ptt_pages = gr.Number(value=3, label=\"å¾€å‰çˆ¬å–é æ•¸\", precision=0)\n",
        "            ptt_min_push = gr.Number(value=0, label=\"æœ€ä½æ¨æ–‡æ•¸\", precision=0)\n",
        "            ptt_keyword = gr.Textbox(label=\"æ¨™é¡Œé—œéµå­—ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "        btn_crawl_ptt = gr.Button(\"ğŸ•·ï¸ é–‹å§‹çˆ¬å– PTT\")\n",
        "        msg_crawl_ptt = gr.Markdown() # é¡¯ç¤ºçˆ¬èŸ²çµæœè¨Šæ¯\n",
        "        btn_refresh_ptt = gr.Button(\"ğŸ”„ å¾ Sheet è¼‰å…¥å·²çˆ¬å–æ–‡ç« \")\n",
        "        msg_refresh_ptt = gr.Markdown()\n",
        "        grid_ptt_posts = gr.Dataframe(value=ptt_posts_df, label=\"å·²çˆ¬å–çš„ PTT æ–‡ç«  (ä¾†è‡ª Sheet)\", interactive=False)\n",
        "\n",
        "        gr.Markdown(\"### å¾ Google Sheet è®€å–é–‹æ”¾å¼å›ç­”\")\n",
        "        # é€™è£¡ä½¿ç”¨ Textbox è®“ä½¿ç”¨è€…è¼¸å…¥å·¥ä½œè¡¨åç¨±å’Œæ¬„ä½åç¨±ï¼Œè€Œä¸æ˜¯å¯«æ­»è®Šæ•¸\n",
        "        input_sheet_name = gr.Textbox(label=\"è¼¸å…¥è³‡æ–™æ‰€åœ¨å·¥ä½œè¡¨åç¨±\", value=INPUT_WORKSHEET_NAME) # ä½¿ç”¨é è¨­å€¼\n",
        "        input_col_name = gr.Textbox(label=\"é–‹æ”¾å¼å›ç­”æ¬„ä½åç¨±\", value=INPUT_HEADER[1]) # ä½¿ç”¨é è¨­å€¼\n",
        "        btn_read_sheet = gr.Button(\"ğŸ“¥ å¾ Sheet è®€å–é–‹æ”¾å¼å›ç­”\")\n",
        "        msg_read_sheet = gr.Markdown() # é¡¯ç¤ºè®€å–çµæœè¨Šæ¯\n",
        "        grid_input_responses = gr.Dataframe(value=input_responses_df, label=\"å¾ Sheet è®€å–çš„é–‹æ”¾å¼å›ç­”\", interactive=False)\n",
        "\n",
        "\n",
        "    # --- æ–‡æœ¬åˆ†æèˆ‡ AI åˆ†é  ---\n",
        "    with gr.Tab(\"æ–‡æœ¬åˆ†æèˆ‡ AI æ‘˜è¦\"):\n",
        "        gr.Markdown(\"### æ–‡æœ¬åˆ†æè¨­å®š\")\n",
        "        data_source_radio = gr.Radio([\"PTT æ–‡ç«  (å·²çˆ¬å–)\", \"é–‹æ”¾å¼å›ç­” (å¾ Sheet è®€å–)\"], label=\"é¸æ“‡åˆ†æè³‡æ–™ä¾†æº\", value=\"é–‹æ”¾å¼å›ç­” (å¾ Sheet è®€å–)\")\n",
        "        analysis_topk = gr.Number(value=50, label=\"è¼¸å‡º Top K é—œéµè©\", precision=0)\n",
        "        analysis_min_df = gr.Number(value=2, label=\"æœ€ä½æ–‡ä»¶é »ç‡ (Min DF)\", precision=0)\n",
        "        btn_analyze = gr.Button(\"ğŸ”¬ åŸ·è¡Œæ–‡æœ¬åˆ†æèˆ‡ AI ç”Ÿæˆ\") # å°‡åˆ†æå’Œ AI ç”Ÿæˆåˆä½µåˆ°ä¸€å€‹æŒ‰éˆ•\n",
        "        msg_analyze = gr.Markdown() # é¡¯ç¤ºåˆ†æçµæœè¨Šæ¯\n",
        "\n",
        "        gr.Markdown(\"### æ–‡æœ¬åˆ†æå ±å‘Šï¼ˆé—œéµè©èˆ‡é›™è©æ­é…ï¼‰\")\n",
        "        out_analysis_report = gr.Markdown() # é¡¯ç¤º Markdown æ ¼å¼çš„åˆ†æå ±å‘Š\n",
        "        grid_terms = gr.Dataframe(value=terms_df, label=\"é—œéµè©åˆ†æçµæœ (å·²å¯«å…¥ Sheet)\", interactive=False)\n",
        "\n",
        "        gr.Markdown(\"### AI ç”Ÿæˆæ´å¯Ÿèˆ‡çµè«–\")\n",
        "        out_ai_insights = gr.Markdown(\"### æ´å¯Ÿæ‘˜è¦\\nï¼ˆå¾…ç”Ÿæˆï¼‰\") # é¡¯ç¤º AI æ´å¯Ÿ\n",
        "        out_ai_conclusion = gr.Markdown(\"### çµè«–\\nï¼ˆå¾…ç”Ÿæˆï¼‰\") # é¡¯ç¤º AI çµè«–\n",
        "\n",
        "\n",
        "    # === ç¶å®šä»‹é¢å…ƒä»¶èˆ‡å¾Œç«¯å‡½å¼ ===\n",
        "\n",
        "    # è³‡æ–™ç²å–åˆ†é çš„æŒ‰éˆ•ç¶å®š\n",
        "    btn_crawl_ptt.click(run_ptt_crawl, inputs=[ptt_pages, ptt_min_push, ptt_keyword], outputs=[msg_crawl_ptt, grid_ptt_posts])\n",
        "    btn_refresh_ptt.click(refresh_ptt_posts, outputs=[grid_ptt_posts, msg_refresh_ptt])\n",
        "    # å‘¼å« refresh_input_responsesï¼Œå‚³å…¥ä½¿ç”¨è€…åœ¨ä»‹é¢è¼¸å…¥çš„å·¥ä½œè¡¨åç¨±å’Œæ¬„ä½åç¨±\n",
        "    btn_read_sheet.click(refresh_input_responses, inputs=[input_sheet_name, input_col_name], outputs=[grid_input_responses, msg_read_sheet])\n",
        "\n",
        "    # æ–‡æœ¬åˆ†æèˆ‡ AI åˆ†é çš„æŒ‰éˆ•ç¶å®š\n",
        "    # é»æ“Šåˆ†ææŒ‰éˆ•æ™‚ï¼Œå‘¼å« perform_analysis_and_aiï¼Œå°‡çµæœæ›´æ–°åˆ°å°æ‡‰çš„ä»‹é¢å€åŸŸ\n",
        "    btn_analyze.click(\n",
        "        perform_analysis_and_ai,\n",
        "        inputs=[data_source_radio, input_col_name, analysis_topk, analysis_min_df], # æ³¨æ„é€™è£¡ input_col_name æ˜¯å¾ä»‹é¢è®€å–çš„å€¼\n",
        "        outputs=[msg_analyze, grid_terms, out_analysis_report, out_ai_insights, out_ai_conclusion] # å°‡æ‰€æœ‰è¼¸å‡ºç¶å®šåˆ°ä»‹é¢å…ƒä»¶\n",
        "    )\n",
        "\n",
        "\n",
        "# å•Ÿå‹• Gradio ä»‹é¢\n",
        "demo.launch(debug=True) # debug=True å¯ä»¥çœ‹åˆ°æ›´å¤šåŸ·è¡Œç´°ç¯€"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eb6cafdeda94d36da5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eb6cafdeda94d36da5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â„¹ï¸ å·¥ä½œè¡¨ 'ptt_movie_posts' ç‚ºç©ºæˆ–ç„¡æ³•è®€å– DataFrameã€‚\n",
            "å˜—è©¦å°‡ 47 ç­†è³‡æ–™å¯«å…¥å·¥ä½œè¡¨ 'ptt_movie_posts'...\n",
            "âœ… å·²å°‡ 47 ç­†è³‡æ–™å¯«å…¥å·¥ä½œè¡¨ 'ptt_movie_posts'ã€‚\n",
            "âœ… å¾å·¥ä½œè¡¨ 'ptt_movie_posts' è®€å– 47 ç­†è³‡æ–™ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å˜—è©¦å°‡ 50 ç­†è³‡æ–™å¯«å…¥å·¥ä½œè¡¨ 'åˆ†æçµæœ'...\n",
            "âœ… å·²å°‡ 50 ç­†è³‡æ–™å¯«å…¥å·¥ä½œè¡¨ 'åˆ†æçµæœ'ã€‚\n"
          ]
        }
      ]
    }
  ]
}